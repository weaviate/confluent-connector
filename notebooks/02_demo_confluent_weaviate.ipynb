{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use the confluent-weaviate connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import weaviate\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup weaviate (embedded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(embedded_options=weaviate.embedded.EmbeddedOptions())\n",
    "\n",
    "client.schema.delete_all()\n",
    "weaviate_url = client._connection.url\n",
    "weaviate_host = weaviate_url.split(\"://\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_packages = [\n",
    "    \"org.apache.spark:spark-avro_2.12:3.4.1\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\",\n",
    "]\n",
    "\n",
    "CONFLUENT_WEAVIATE_JAR = \"../target/scala-2.12/confluent-connector_2.12-3.4.0_0.0.1.jar\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"demo-confluent-weaviate-integration\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(jar_packages))\n",
    "    .config(\"spark.jars\", CONFLUENT_WEAVIATE_JAR)\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the creds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confluentClusterName = os.environ.get(\"CONFLUENT_CLUSTER_NAME\")\n",
    "confluentBootstrapServers = os.environ.get(\"CONFLUENT_BOOTSTRAP_SERVERS\")\n",
    "confluentTopicName = os.environ.get(\"CONFLUENT_TOPIC_NAME\")\n",
    "schemaRegistryUrl = os.environ.get(\"SCHEMA_REGISTRY_URL\")\n",
    "confluentApiKey = os.environ.get(\"CONFLUENT_API_KEY\")\n",
    "confluentSecret = os.environ.get(\"CONFLUENT_SECRET\")\n",
    "confluentRegistryApiKey = os.environ.get(\"CONFLUENT_REGISTRY_API_KEY\")\n",
    "confluentRegistrySecret = os.environ.get(\"CONFLUENT_REGISTRY_SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schema in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/it/resources/schema.json\", \"r\") as f:\n",
    "    weaviate_schema = json.load(f)\n",
    "\n",
    "client.schema.create_class(weaviate_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Structured Streaming `DataFrame` to read streaming data from a Confluent Kafka topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstreamDF = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"subscribe\", confluentTopicName)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\n",
    "        \"kafka.sasl.jaas.config\",\n",
    "        \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(\n",
    "            confluentApiKey, confluentSecret\n",
    "        ),\n",
    "    )\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"name\", \"clickStreamReadFromConfluent\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run on each microbatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows_processed = 0\n",
    "\n",
    "\n",
    "def f(df, batch_id):\n",
    "    global total_rows_processed\n",
    "    row_count = df.count()\n",
    "    total_rows_processed += row_count\n",
    "\n",
    "    print(f\"Number of rows in the batch with batch id {batch_id}: {row_count}\")\n",
    "    df.write.format(\"io.weaviate.confluent.Weaviate\").option(\"batchsize\", 200).option(\n",
    "        \"scheme\", \"http\"\n",
    "    ).option(\"host\", weaviate_host).option(\n",
    "        \"className\", weaviate_schema[\"class\"]\n",
    "    ).option(\n",
    "        \"schemaRegistryUrl\", ...\n",
    "    ).option(\n",
    "        \"schemaRegistryApiKey\", ...\n",
    "    ).option(\n",
    "        \"schemaRegistryApiSecret\", ...\n",
    "    ).mode(\n",
    "        \"append\"\n",
    "    ).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start writinng the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    clickstreamDF.writeStream.foreachBatch(f)\n",
    "    .queryName(\"write_stream_to_weaviate\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop writing after 30 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not gracefully shutdown the stream!\n",
    "# easiest way to gracefully shutdown is to pause the source connector\n",
    "time.sleep(30)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number of rows processed and the number of objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.query.aggregate(weaviate_schema[\"class\"]).with_meta_count().do()\n",
    "total_objects_in_weaviate = results[\"data\"][\"Aggregate\"][weaviate_schema[\"class\"]][0][\n",
    "    \"meta\"\n",
    "][\"count\"]\n",
    "\n",
    "assert (\n",
    "    total_rows_processed == total_objects_in_weaviate\n",
    "), f\"Total rows processed {total_rows_processed} does not match total objects in weaviate {total_objects_in_weaviate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.data_object.get(class_name=weaviate_schema[\"class\"], limit=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
