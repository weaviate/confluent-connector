{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use the confluent-weaviate connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import weaviate\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup weaviate (embedded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /home/vscode/.cache/weaviate-embedded: process ID 6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2023-09-07T12:16:38Z\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2023-09-07T12:16:38Z\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"clickstream_YQr0XTsK5Id5\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-09-07T12:16:38Z\",\"took\":49167}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2023-09-07T12:16:38Z\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50051\",\"time\":\"2023-09-07T12:16:38Z\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:6666\",\"time\":\"2023-09-07T12:16:38Z\"}\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(embedded_options=weaviate.embedded.EmbeddedOptions())\n",
    "\n",
    "client.schema.delete_all()\n",
    "weaviate_url = client._connection.url\n",
    "weaviate_host = weaviate_url.split(\"://\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/vscode/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-30ca936e-0ed3-4576-bc4f-5ea899046d9b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.4.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 310ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-30ca936e-0ed3-4576-bc4f-5ea899046d9b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/6ms)\n",
      "23/09/07 12:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "jar_packages = [\n",
    "    \"org.apache.spark:spark-avro_2.12:3.4.1\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\",\n",
    "]\n",
    "\n",
    "CONFLUENT_WEAVIATE_JAR = \"../target/scala-2.12/confluent-connector_2.12-3.4.0_0.0.1.jar\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"demo-confluent-weaviate-integration\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(jar_packages))\n",
    "    .config(\"spark.jars\", CONFLUENT_WEAVIATE_JAR)\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the creds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "confluentClusterName = os.environ.get(\"CONFLUENT_CLUSTER_NAME\")\n",
    "confluentBootstrapServers = os.environ.get(\"CONFLUENT_BOOTSTRAP_SERVERS\")\n",
    "confluentTopicName = os.environ.get(\"CONFLUENT_TOPIC_NAME\")\n",
    "schemaRegistryUrl = os.environ.get(\"SCHEMA_REGISTRY_URL\")\n",
    "confluentApiKey = os.environ.get(\"CONFLUENT_API_KEY\")\n",
    "confluentSecret = os.environ.get(\"CONFLUENT_SECRET\")\n",
    "confluentRegistryApiKey = os.environ.get(\"CONFLUENT_REGISTRY_API_KEY\")\n",
    "confluentRegistrySecret = os.environ.get(\"CONFLUENT_REGISTRY_SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schema in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"clickstream_WLF7iJhViZaL\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-09-07T12:16:42Z\",\"took\":46000}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/it/resources/schema.json\", \"r\") as f:\n",
    "    weaviate_schema = json.load(f)\n",
    "\n",
    "client.schema.create_class(weaviate_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Structured Streaming `DataFrame` to read streaming data from a Confluent Kafka topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstreamDF = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"subscribe\", confluentTopicName)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\n",
    "        \"kafka.sasl.jaas.config\",\n",
    "        \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(\n",
    "            confluentApiKey, confluentSecret\n",
    "        ),\n",
    "    )\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"name\", \"clickStreamReadFromConfluent\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run on each microbatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows_processed = 0\n",
    "\n",
    "\n",
    "def f(df, batch_id):\n",
    "    global total_rows_processed\n",
    "    row_count = df.count()\n",
    "    total_rows_processed += row_count\n",
    "\n",
    "    print(f\"Number of rows in the batch with batch id {batch_id}: {row_count}\")\n",
    "    df.write.format(\"io.weaviate.confluent.Weaviate\").option(\"batchsize\", 200).option(\n",
    "        \"scheme\", \"http\"\n",
    "    ).option(\"host\", weaviate_host).option(\n",
    "        \"className\", weaviate_schema[\"class\"]\n",
    "    ).option(\n",
    "        \"schemaRegistryUrl\", schemaRegistryUrl\n",
    "    ).option(\n",
    "        \"schemaRegistryApiKey\", confluentRegistryApiKey\n",
    "    ).option(\n",
    "        \"schemaRegistryApiSecret\", confluentRegistrySecret\n",
    "    ).mode(\n",
    "        \"append\"\n",
    "    ).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start writinng the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 12:17:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-60068927-480e-4d1e-98ef-3519c8677c63. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/09/07 12:17:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 12:17:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 0: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 1: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 2: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 3: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 4: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 5: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 6: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    clickstreamDF.writeStream.foreachBatch(f)\n",
    "    .queryName(\"write_stream_to_weaviate\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop writing after 15 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not gracefully shutdown the stream!\n",
    "# easiest way to gracefully shutdown is to pause the source connector\n",
    "time.sleep(15)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number of rows processed and the number of objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.query.aggregate(weaviate_schema[\"class\"]).with_meta_count().do()\n",
    "total_objects_in_weaviate = results[\"data\"][\"Aggregate\"][weaviate_schema[\"class\"]][0][\n",
    "    \"meta\"\n",
    "][\"count\"]\n",
    "\n",
    "assert (\n",
    "    total_rows_processed == total_objects_in_weaviate\n",
    "), f\"Total rows processed {total_rows_processed} does not match total objects in weaviate {total_objects_in_weaviate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deprecations': [],\n",
       " 'objects': [{'class': 'Clickstream',\n",
       "   'creationTimeUnix': 1694089043857,\n",
       "   'id': '0394974f-e210-4c7a-8a5c-d1f8aac2b1ea',\n",
       "   'lastUpdateTimeUnix': 1694089043857,\n",
       "   'properties': {'_kafka_key': '179742',\n",
       "    '_kafka_offset': 30217,\n",
       "    '_kafka_partition': 4,\n",
       "    '_kafka_schemaId': 100002,\n",
       "    '_kafka_timestamp': '2023-09-07T12:17:20.872Z',\n",
       "    '_kafka_timestampType': 0,\n",
       "    '_kafka_topic': 'clickstreams-users',\n",
       "    'city': 'New York',\n",
       "    'first_name': 'Greta',\n",
       "    'last_name': 'Romagosa',\n",
       "    'level': 'Silver',\n",
       "    'registered_at': 1447306697329,\n",
       "    'user_id': 179742,\n",
       "    'username': 'AlanGreta_GG66'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Clickstream',\n",
       "   'creationTimeUnix': 1694089043336,\n",
       "   'id': '09415e3d-6091-44c9-9d35-3a2b06ecd724',\n",
       "   'lastUpdateTimeUnix': 1694089043336,\n",
       "   'properties': {'_kafka_key': '179741',\n",
       "    '_kafka_offset': 30169,\n",
       "    '_kafka_partition': 2,\n",
       "    '_kafka_schemaId': 100002,\n",
       "    '_kafka_timestamp': '2023-09-07T12:17:20.385Z',\n",
       "    '_kafka_timestampType': 0,\n",
       "    '_kafka_topic': 'clickstreams-users',\n",
       "    'city': 'Raleigh',\n",
       "    'first_name': 'Ferd',\n",
       "    'last_name': 'Romagosa',\n",
       "    'level': 'Silver',\n",
       "    'registered_at': 1492451514725,\n",
       "    'user_id': 179741,\n",
       "    'username': 'GlenAlan_23344'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Clickstream',\n",
       "   'creationTimeUnix': 1694089038494,\n",
       "   'id': '0cdfed12-5a2c-4955-9164-0d1af39c569c',\n",
       "   'lastUpdateTimeUnix': 1694089038494,\n",
       "   'properties': {'_kafka_key': '179728',\n",
       "    '_kafka_offset': 30163,\n",
       "    '_kafka_partition': 2,\n",
       "    '_kafka_schemaId': 100002,\n",
       "    '_kafka_timestamp': '2023-09-07T12:17:12.657Z',\n",
       "    '_kafka_timestampType': 0,\n",
       "    '_kafka_topic': 'clickstreams-users',\n",
       "    'city': 'New York',\n",
       "    'first_name': 'Woodrow',\n",
       "    'last_name': 'Rockhill',\n",
       "    'level': 'Silver',\n",
       "    'registered_at': 1451657504877,\n",
       "    'user_id': 179728,\n",
       "    'username': 'AlanGreta_GG66'},\n",
       "   'vectorWeights': None}],\n",
       " 'totalResults': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.data_object.get(class_name=weaviate_schema[\"class\"], limit=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
