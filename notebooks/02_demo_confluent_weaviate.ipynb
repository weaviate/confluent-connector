{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook shows how to use the confluent-weaviate connector with an [embedded Weaviate](https://weaviate.io/developers/weaviate/installation/embedded) instance.\n",
    "\n",
    "## Data Stream Source\n",
    "The demonstration data stream is generated using the [Datagen Source Connector](https://docs.confluent.io/cloud/current/connectors/cc-datagen-source.html). Specifically, the `clickstream_users_schema.avro` template is employed for this purpose. You can find the template [here](https://github.com/confluentinc/kafka-connect-datagen/blob/master/src/main/resources/clickstream_users_schema.avro).\n",
    "\n",
    "## Supported Message Properties\n",
    "Currently, the connector supports messages with the following properties:\n",
    "- **Key**: Must be a string.\n",
    "- **Value**: Serialized using Avro.\n",
    "- **Timestamps**: Unix timestamps in milliseconds.\n",
    "\n",
    "Ensure your data stream adheres to these properties for successful integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import weaviate\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup weaviate (embedded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /home/vscode/.cache/weaviate-embedded: process ID 96296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2023-09-08T11:34:55Z\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2023-09-08T11:34:55Z\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"clickstream_cvECIvOhVPZT\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-09-08T11:34:55Z\",\"took\":83750}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2023-09-08T11:34:55Z\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50051\",\"time\":\"2023-09-08T11:34:55Z\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:6666\",\"time\":\"2023-09-08T11:34:55Z\"}\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(embedded_options=weaviate.embedded.EmbeddedOptions())\n",
    "\n",
    "client.schema.delete_all()\n",
    "weaviate_url = client._connection.url\n",
    "weaviate_host = weaviate_url.split(\"://\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/vscode/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-62054124-421a-4697-8dbd-f511ae6b720e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.4.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 280ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-62054124-421a-4697-8dbd-f511ae6b720e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/5ms)\n",
      "23/09/08 11:34:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "jar_packages = [\n",
    "    \"org.apache.spark:spark-avro_2.12:3.4.1\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\",\n",
    "]\n",
    "\n",
    "CONFLUENT_WEAVIATE_JAR = \"../target/scala-2.12/confluent-connector_2.12-3.4.0_0.0.1.jar\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"demo-confluent-weaviate-integration\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(jar_packages))\n",
    "    .config(\"spark.jars\", CONFLUENT_WEAVIATE_JAR)\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the creds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "confluentClusterName = os.environ.get(\"CONFLUENT_CLUSTER_NAME\")\n",
    "confluentBootstrapServers = os.environ.get(\"CONFLUENT_BOOTSTRAP_SERVERS\")\n",
    "confluentTopicName = os.environ.get(\"CONFLUENT_TOPIC_NAME\")\n",
    "schemaRegistryUrl = os.environ.get(\"CONFLUENT_SCHEMA_REGISTRY_URL\")\n",
    "confluentApiKey = os.environ.get(\"CONFLUENT_API_KEY\")\n",
    "confluentSecret = os.environ.get(\"CONFLUENT_SECRET\")\n",
    "confluentRegistryApiKey = os.environ.get(\"CONFLUENT_REGISTRY_API_KEY\")\n",
    "confluentRegistrySecret = os.environ.get(\"CONFLUENT_REGISTRY_SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schema in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"clickstream_luwwRQEGH2R3\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-09-08T11:34:59Z\",\"took\":42750}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../src/it/resources/schema.json\", \"r\") as f:\n",
    "    weaviate_schema = json.load(f)\n",
    "\n",
    "client.schema.create_class(weaviate_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Structured Streaming `DataFrame` to read streaming data from a Confluent Kafka topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstreamDF = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"subscribe\", confluentTopicName)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\n",
    "        \"kafka.sasl.jaas.config\",\n",
    "        \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(\n",
    "            confluentApiKey, confluentSecret\n",
    "        ),\n",
    "    )\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"name\", \"clickStreamReadFromConfluent\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run on each microbatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows_processed = 0\n",
    "\n",
    "\n",
    "def f(df, batch_id):\n",
    "    global total_rows_processed\n",
    "    row_count = df.count()\n",
    "    total_rows_processed += row_count\n",
    "\n",
    "    print(f\"Number of rows in the batch with batch id {batch_id}: {row_count}\")\n",
    "    df.write.format(\"io.weaviate.confluent.Weaviate\").option(\"batchsize\", 200).option(\n",
    "        \"scheme\", \"http\"\n",
    "    ).option(\"host\", weaviate_host).option(\n",
    "        \"className\", weaviate_schema[\"class\"]\n",
    "    ).option(\n",
    "        \"schemaRegistryUrl\", schemaRegistryUrl\n",
    "    ).option(\n",
    "        \"schemaRegistryApiKey\", confluentRegistryApiKey\n",
    "    ).option(\n",
    "        \"schemaRegistryApiSecret\", confluentRegistrySecret\n",
    "    ).mode(\n",
    "        \"append\"\n",
    "    ).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start writinng the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 11:35:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b54d9113-db3a-4b85-b14b-19bdb66cf4d1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/09/08 11:35:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 11:35:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 0: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 1: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 2: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 3: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 4: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    clickstreamDF.writeStream.foreachBatch(f)\n",
    "    .queryName(\"write_stream_to_weaviate\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop writing after 15 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not gracefully shutdown the stream!\n",
    "# easiest way to gracefully shutdown is to pause the source connector\n",
    "time.sleep(15)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number of rows processed and the number of objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.query.aggregate(weaviate_schema[\"class\"]).with_meta_count().do()\n",
    "total_objects_in_weaviate = results[\"data\"][\"Aggregate\"][weaviate_schema[\"class\"]][0][\n",
    "    \"meta\"\n",
    "][\"count\"]\n",
    "\n",
    "assert (\n",
    "    total_rows_processed == total_objects_in_weaviate\n",
    "), f\"Total rows processed {total_rows_processed} does not match total objects in weaviate {total_objects_in_weaviate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deprecations': [],\n",
       " 'objects': [{'class': 'Clickstream',\n",
       "   'creationTimeUnix': 1694172920668,\n",
       "   'id': '01eafe7b-a2aa-4610-acb4-a0ff499b92ee',\n",
       "   'lastUpdateTimeUnix': 1694172920668,\n",
       "   'properties': {'_kafka_key': '200377',\n",
       "    '_kafka_offset': 33454,\n",
       "    '_kafka_partition': 0,\n",
       "    '_kafka_schemaId': 100002,\n",
       "    '_kafka_timestamp': '2023-09-08T11:35:15.241Z',\n",
       "    '_kafka_timestampType': 0,\n",
       "    '_kafka_topic': 'clickstreams-users',\n",
       "    'city': 'Frankfurt',\n",
       "    'first_name': 'Curran',\n",
       "    'last_name': 'Tomini',\n",
       "    'level': 'Gold',\n",
       "    'registered_at': 1483222736170,\n",
       "    'user_id': 200377,\n",
       "    'username': 'Roberto_123'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Clickstream',\n",
       "   'creationTimeUnix': 1694172923212,\n",
       "   'id': '07497374-2bda-4ada-9cf3-d87107c3b2d2',\n",
       "   'lastUpdateTimeUnix': 1694172923212,\n",
       "   'properties': {'_kafka_key': '200389',\n",
       "    '_kafka_offset': 33317,\n",
       "    '_kafka_partition': 1,\n",
       "    '_kafka_schemaId': 100002,\n",
       "    '_kafka_timestamp': '2023-09-08T11:35:20.426Z',\n",
       "    '_kafka_timestampType': 0,\n",
       "    '_kafka_topic': 'clickstreams-users',\n",
       "    'city': 'San Francisco',\n",
       "    'first_name': 'Arlyne',\n",
       "    'last_name': 'Vanyard',\n",
       "    'level': 'Platinum',\n",
       "    'registered_at': 1446003917171,\n",
       "    'user_id': 200389,\n",
       "    'username': 'ArlyneW8ter'},\n",
       "   'vectorWeights': None},\n",
       "  {'class': 'Clickstream',\n",
       "   'creationTimeUnix': 1694172923212,\n",
       "   'id': '0b51efd0-9b3a-4aeb-8731-6f386c71aa3c',\n",
       "   'lastUpdateTimeUnix': 1694172923212,\n",
       "   'properties': {'_kafka_key': '200388',\n",
       "    '_kafka_offset': 33658,\n",
       "    '_kafka_partition': 4,\n",
       "    '_kafka_schemaId': 100002,\n",
       "    '_kafka_timestamp': '2023-09-08T11:35:20.03Z',\n",
       "    '_kafka_timestampType': 0,\n",
       "    '_kafka_topic': 'clickstreams-users',\n",
       "    'city': 'New York',\n",
       "    'first_name': 'Abdel',\n",
       "    'last_name': 'Garrity',\n",
       "    'level': 'Silver',\n",
       "    'registered_at': 1460593910010,\n",
       "    'user_id': 200388,\n",
       "    'username': 'Oriana_70'},\n",
       "   'vectorWeights': None}],\n",
       " 'totalResults': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.data_object.get(class_name=weaviate_schema[\"class\"], limit=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
