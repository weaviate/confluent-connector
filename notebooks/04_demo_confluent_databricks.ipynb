{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da2009e-8b9d-4a5b-9dff-04a9c7b47873",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook shows how to use the confluent-weaviate connector with [Weaviate Cloud Services](https://weaviate.io/pricing) and [Databricks](https://databricks.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62be4fd-c398-4ba8-ab23-0add900f319b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e77677-e371-412c-b980-0686cae36ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Install the weaviate client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5400973a-4ace-4d1b-9ba2-f9e37c316fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weaviate-client\r\n",
      "  Using cached weaviate_client-3.24.1-py3-none-any.whl (107 kB)\r\n",
      "Collecting requests<3.0.0,>=2.30.0\r\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\r\n",
      "Collecting validators<1.0.0,>=0.21.2\r\n",
      "  Using cached validators-0.22.0-py3-none-any.whl (26 kB)\r\n",
      "Collecting authlib<2.0.0,>=1.2.1\r\n",
      "  Using cached Authlib-1.2.1-py2.py3-none-any.whl (215 kB)\r\n",
      "Requirement already satisfied: cryptography>=3.2 in /databricks/python3/lib/python3.9/site-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (3.4.8)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.2->authlib<2.0.0,>=1.2.1->weaviate-client) (2.21)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2021.10.8)\r\n",
      "Installing collected packages: validators, requests, authlib, weaviate-client\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.27.1\r\n",
      "    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-a730a752-2104-4686-8b68-5d410fc80cc2\r\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\r\n",
      "Successfully installed authlib-1.2.1 requests-2.31.0 validators-0.22.0 weaviate-client-3.24.1\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-a730a752-2104-4686-8b68-5d410fc80cc2/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0530c0e0-91e7-464f-be60-fd25a909ab17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3742d6f5-6d60-4210-be9a-a58d530cc819",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import weaviate\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062b5106-acae-404a-bbde-02f3f7b0bd03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66209c3c-9789-4d9a-9720-e7fa1bca8b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Setup weaviate client to connect to Weaviate Cloud Services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c117390d-8c05-4aaa-91d5-a6ecfdc873a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wcs_url = dbutils.secrets.get(\"demo-confluent-connector\", \"WCS_URL\")\n",
    "wcs_api_key = dbutils.secrets.get(\"demo-confluent-connector\", \"WCS_API_KEY\")\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=wcs_url,\n",
    "    auth_client_secret=weaviate.AuthApiKey(wcs_api_key),\n",
    ")\n",
    "\n",
    "client.schema.delete_all()\n",
    "weaviate_url = client._connection.url\n",
    "weaviate_host = weaviate_url.split(\"://\")[1]\n",
    "\n",
    "token = client._connection._headers[\"authorization\"]\n",
    "weaviate_api_key = token.split(\"Bearer \")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4edb75f3-b87e-47f6-9f31-e855ee8749a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Setup the spark session:\n",
    "\n",
    "Make sure you have installed the following libraries into your cluster:\n",
    "\n",
    "1. org.apache.spark:spark-avro_2.12:3.4.1\n",
    "2. org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\n",
    "3. confluent-connector_2.12-3.4.0_0.0.1.jar\n",
    "\n",
    "Reference:\n",
    "* [Libraries](https://docs.databricks.com/en/libraries/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f1905d-3b84-4719-a851-49c91212d4ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Grab the Confluent Cloud Platform-related credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649d1735-a8c3-49bd-be9a-d3fe6739f929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "confluentClusterName = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_CLUSTER_NAME\")\n",
    "confluentBootstrapServers = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_BOOTSTRAP_SERVERS\")\n",
    "confluentTopicName = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_TOPIC_NAME\")\n",
    "schemaRegistryUrl = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_SCHEMA_REGISTRY_URL\")\n",
    "confluentApiKey = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_API_KEY\")\n",
    "confluentSecret = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_SECRET\")\n",
    "confluentRegistryApiKey = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_REGISTRY_API_KEY\")\n",
    "confluentRegistrySecret = dbutils.secrets.get(\"demo-confluent-connector\", \"CONFLUENT_REGISTRY_SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadfad87-9406-436f-bb11-31b268bbd582",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566ee326-7572-43f7-8d2f-d39eb8e41aaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a schema in WCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5778ee-1054-4cc6-858a-fa37d135226d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weaviate_schema = {\n",
    "    \"class\": \"Clickstream\",\n",
    "    \"description\": \"A record of user clicks on a website\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"_kafka_key\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The key of the Kafka message\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"_kafka_topic\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The topic of the Kafka message\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"_kafka_partition\",\n",
    "            \"dataType\": [\n",
    "                \"int\"\n",
    "            ],\n",
    "            \"description\": \"The partition of the Kafka message\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"_kafka_offset\",\n",
    "            \"dataType\": [\n",
    "                \"int\"\n",
    "            ],\n",
    "            \"description\": \"The offset of the Kafka message\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"_kafka_timestamp\",\n",
    "            \"dataType\": [\n",
    "                \"date\"\n",
    "            ],\n",
    "            \"description\": \"The timestamp of the Kafka message\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"_kafka_timestampType\",\n",
    "            \"dataType\": [\n",
    "                \"int\"\n",
    "            ],\n",
    "            \"description\": \"The timestamp type of the Kafka message\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"_kafka_schema_id\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The schema ID of the Kafka message value\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"user_id\",\n",
    "            \"dataType\": [\n",
    "                \"int\"\n",
    "            ],\n",
    "            \"description\": \"The ID of the user who clicked\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"username\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The username of the user who clicked\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"registered_at\",\n",
    "            \"dataType\": [\n",
    "                \"int\"\n",
    "            ],\n",
    "            \"description\": \"The timestamp when the user registered\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"first_name\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The first name of the user who clicked\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"last_name\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The last name of the user who clicked\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"city\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The city where the user who clicked is located\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"level\",\n",
    "            \"dataType\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"description\": \"The level of the user who clicked\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6330bd-9a2d-4418-bdee-804114aec63d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.schema.create_class(weaviate_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fede16a-bcd7-4ae9-820c-2aba1523c229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a Spark Structured Streaming `DataFrame` to read streaming data from a Kafka topic from Confluent Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4fba2a-c6a1-4400-94d0-b60cb9014880",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clickstreamDF = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"subscribe\", confluentTopicName)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\n",
    "        \"kafka.sasl.jaas.config\",\n",
    "        \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(\n",
    "            confluentApiKey, confluentSecret\n",
    "        ),\n",
    "    )\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"name\", \"clickStreamReadFromConfluent\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d4f4f8-90ae-464b-bc26-a72cff552757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Define a function to run on each microbatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69a9262-5244-41bf-bd98-a46211ff4d7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_rows_processed = 0\n",
    "\n",
    "\n",
    "def f(df, batch_id):\n",
    "    global total_rows_processed\n",
    "    row_count = df.count()\n",
    "    total_rows_processed += row_count\n",
    "\n",
    "    print(f\"Number of rows in the batch with batch id {batch_id}: {row_count}\")\n",
    "    df.write.format(\"io.weaviate.confluent.Weaviate\").option(\"batchsize\", 200).option(\n",
    "        \"scheme\", \"http\"\n",
    "    ).option(\"host\", weaviate_host).option(\"apiKey\", weaviate_api_key).option(\n",
    "        \"className\", weaviate_schema[\"class\"]\n",
    "    ).option(\n",
    "        \"schemaRegistryUrl\", schemaRegistryUrl\n",
    "    ).option(\n",
    "        \"schemaRegistryApiKey\", confluentRegistryApiKey\n",
    "    ).option(\n",
    "        \"schemaRegistryApiSecret\", confluentRegistrySecret\n",
    "    ).mode(\n",
    "        \"append\"\n",
    "    ).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0892d1-daa6-4fe7-8191-c15e3ee958e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Start writing the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea6b9ea-83e6-4fed-ad02-3a5ae7e6e4a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    clickstreamDF.writeStream.foreachBatch(f)\n",
    "    .queryName(\"write_stream_to_weaviate\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3dc91c-9f88-4ec6-bd0d-8a68cb65e161",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Stop writing after 15 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde2fa42-9e86-4f6f-adc2-ed372fa8ff85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the batch with batch id 0: 1\n",
      "Number of rows in the batch with batch id 1: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 119, in call\n",
      "    raise e\n",
      "  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 116, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"<command-3510017430435041>\", line 6, in f\n",
      "    row_count = df.count()\n",
      "  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 1214, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 228, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1717.count.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1000)\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1308)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:507)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1173)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1170)\n",
      "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2750)\n",
      "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:297)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
      "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n",
      "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n",
      "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n",
      "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n",
      "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n",
      "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n",
      "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n",
      "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:541)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
      "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n",
      "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n",
      "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
      "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:479)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3561)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3560)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:417)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:128)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:367)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3560)\n",
      "\tat sun.reflect.GeneratedMethodAccessor378.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:257)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy97.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatchLegacy(ForeachBatchSink.scala:121)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.$anonfun$addBatch$2(ForeachBatchSink.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:78)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:971)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:417)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:128)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:966)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:335)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:966)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStreamWithListener$5(MicroBatchExecution.scala:444)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withSchemaEvolution(MicroBatchExecution.scala:1111)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStreamWithListener$2(MicroBatchExecution.scala:441)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:335)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStreamWithListener$1(MicroBatchExecution.scala:399)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStreamWithListener(MicroBatchExecution.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$2(StreamExecution.scala:384)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:412)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:410)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n",
      "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:455)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:440)\n",
      "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)\n",
      "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:69)\n",
      "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:169)\n",
      "\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)\n",
      "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)\n",
      "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)\n",
      "\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)\n",
      "\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:260)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:260)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time.sleep(15)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9558a1-354c-4a30-914c-c724959a1c6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdea95d-76f9-4d02-b586-24df03062fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Compare the number of rows processed and the number of objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c416c74f-86fe-4d57-8273-5f4ac558b443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = client.query.aggregate(weaviate_schema[\"class\"]).with_meta_count().do()\n",
    "total_objects_in_weaviate = results[\"data\"][\"Aggregate\"][weaviate_schema[\"class\"]][0][\n",
    "    \"meta\"\n",
    "][\"count\"]\n",
    "\n",
    "assert (\n",
    "    total_rows_processed == total_objects_in_weaviate\n",
    "), f\"Total rows processed {total_rows_processed} does not match total objects in weaviate {total_objects_in_weaviate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d9f7573-52c2-4f4d-beea-51c4e663f7e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Look at some of the objects in Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c93a200-81cd-41bd-9a7b-ca0bfceddbe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: {'deprecations': [],\n",
      " 'objects': [{'class': 'Clickstream',\n",
      "   'creationTimeUnix': 1694802167098,\n",
      "   'id': '1450d859-f98f-453e-aaf2-4ecad0877345',\n",
      "   'lastUpdateTimeUnix': 1694802167098,\n",
      "   'properties': {'_kafka_key': '203414',\n",
      "    '_kafka_offset': 33782,\n",
      "    '_kafka_partition': 3,\n",
      "    '_kafka_schemaId': 100002,\n",
      "    '_kafka_timestamp': '2023-09-15T18:22:39.691Z',\n",
      "    '_kafka_timestampType': 0,\n",
      "    '_kafka_topic': '[REDACTED]',\n",
      "    'city': 'Frankfurt',\n",
      "    'first_name': 'Reeva',\n",
      "    'last_name': 'Vanyard',\n",
      "    'level': 'Gold',\n",
      "    'registered_at': 1442165147142,\n",
      "    'user_id': 203414,\n",
      "    'username': 'LukeWaters_23'},\n",
      "   'vectorWeights': None},\n",
      "  {'class': 'Clickstream',\n",
      "   'creationTimeUnix': 1694802162605,\n",
      "   'id': '45609b6d-d2fb-4476-833f-c5c4a3d411de',\n",
      "   'lastUpdateTimeUnix': 1694802162605,\n",
      "   'properties': {'_kafka_key': '203412',\n",
      "    '_kafka_offset': 34137,\n",
      "    '_kafka_partition': 4,\n",
      "    '_kafka_schemaId': 100002,\n",
      "    '_kafka_timestamp': '2023-09-15T18:22:38.702Z',\n",
      "    '_kafka_timestampType': 0,\n",
      "    '_kafka_topic': '[REDACTED]',\n",
      "    'city': 'Raleigh',\n",
      "    'first_name': 'Greta',\n",
      "    'last_name': 'Vanyard',\n",
      "    'level': 'Gold',\n",
      "    'registered_at': 1488456744476,\n",
      "    'user_id': 203412,\n",
      "    'username': 'DimitriSchenz88'},\n",
      "   'vectorWeights': None},\n",
      "  {'class': 'Clickstream',\n",
      "   'creationTimeUnix': 1694802162605,\n",
      "   'id': '51f58a36-99a3-4fd1-82c6-ededee061c67',\n",
      "   'lastUpdateTimeUnix': 1694802162605,\n",
      "   'properties': {'_kafka_key': '203412',\n",
      "    '_kafka_offset': 34137,\n",
      "    '_kafka_partition': 4,\n",
      "    '_kafka_schemaId': 100002,\n",
      "    '_kafka_timestamp': '2023-09-15T18:22:38.702Z',\n",
      "    '_kafka_timestampType': 0,\n",
      "    '_kafka_topic': '[REDACTED]',\n",
      "    'city': 'Raleigh',\n",
      "    'first_name': 'Greta',\n",
      "    'last_name': 'Vanyard',\n",
      "    'level': 'Gold',\n",
      "    'registered_at': 1488456744476,\n",
      "    'user_id': 203412,\n",
      "    'username': 'DimitriSchenz88'},\n",
      "   'vectorWeights': None}],\n",
      " 'totalResults': 3}"
     ]
    }
   ],
   "source": [
    "client.data_object.get(class_name=weaviate_schema[\"class\"], limit=3)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_demo_confluent_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
